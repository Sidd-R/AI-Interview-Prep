{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9513301,"sourceType":"datasetVersion","datasetId":5791113},{"sourceId":9516252,"sourceType":"datasetVersion","datasetId":5793406},{"sourceId":9516544,"sourceType":"datasetVersion","datasetId":5793647}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install PyPDF2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PyPDF2 import PdfReader","metadata":{"execution":{"iopub.status.busy":"2024-09-30T10:43:05.049292Z","iopub.execute_input":"2024-09-30T10:43:05.049706Z","iopub.status.idle":"2024-09-30T10:43:05.297022Z","shell.execute_reply.started":"2024-09-30T10:43:05.049664Z","shell.execute_reply":"2024-09-30T10:43:05.296012Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"text = \"\"\npdf_reader = PdfReader('/kaggle/input/tsechack/Vikas_Resume.pdf')\nfor page in pdf_reader.pages:\n    text += page.extract_text()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T10:43:08.843058Z","iopub.execute_input":"2024-09-30T10:43:08.844035Z","iopub.status.idle":"2024-09-30T10:43:08.944458Z","shell.execute_reply.started":"2024-09-30T10:43:08.843989Z","shell.execute_reply":"2024-09-30T10:43:08.943551Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# print(text)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T10:43:09.043838Z","iopub.execute_input":"2024-09-30T10:43:09.044231Z","iopub.status.idle":"2024-09-30T10:43:09.048907Z","shell.execute_reply.started":"2024-09-30T10:43:09.044192Z","shell.execute_reply":"2024-09-30T10:43:09.047907Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\ntorch.random.manual_seed(0)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"microsoft/Phi-3.5-mini-instruct\",\n    device_map=\"cuda\",\n    torch_dtype=\"auto\",\n    trust_remote_code=True,\n)\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")\n\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n)\n\ngeneration_args = {\n    \"max_new_tokens\": 500,\n    \"return_full_text\": False,\n    \"temperature\": None,\n    \"do_sample\": False,\n}","metadata":{"execution":{"iopub.status.busy":"2024-09-30T10:43:13.698146Z","iopub.execute_input":"2024-09-30T10:43:13.698848Z","iopub.status.idle":"2024-09-30T10:43:29.606941Z","shell.execute_reply.started":"2024-09-30T10:43:13.698805Z","shell.execute_reply":"2024-09-30T10:43:29.605984Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14929ef9dd0d43a982de8ec764bf316e"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Starting the Interview","metadata":{}},{"cell_type":"code","source":"list_of_quesn = []","metadata":{"execution":{"iopub.status.busy":"2024-09-30T11:03:51.609180Z","iopub.execute_input":"2024-09-30T11:03:51.609931Z","iopub.status.idle":"2024-09-30T11:03:51.614228Z","shell.execute_reply.started":"2024-09-30T11:03:51.609885Z","shell.execute_reply":"2024-09-30T11:03:51.613269Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"start_question = [\n{\"role\": \"system\", \"content\": \n\"You are a helpful AI assistant.\"},\n    \n{\"role\": \"user\", \"content\": \nf\"\"\"\n{text}\n\nYou have candidate resume above ask one short technical question which gives good start to interview process.\nFormat of the output has to be just one question no explaination needed.\n\"\"\".strip()},\n    \n]\n\noutput = pipe(start_question, **generation_args)\nres = output[0]['generated_text'].strip()\n\nlist_of_quesn.append(res)\n\nprint(res)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T11:03:53.034922Z","iopub.execute_input":"2024-09-30T11:03:53.035302Z","iopub.status.idle":"2024-09-30T11:03:56.352267Z","shell.execute_reply.started":"2024-09-30T11:03:53.035264Z","shell.execute_reply":"2024-09-30T11:03:56.351254Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","output_type":"stream"},{"name":"stdout","text":"What machine learning model did you fine-tune for the forest fire management system, and what was the impact on development time?\n","output_type":"stream"}]},{"cell_type":"code","source":"import io\nfrom google.oauth2 import service_account\nfrom google.cloud import speech\n\nclient_file = '/kaggle/input/google-audio/audio.json'\ncredentials = service_account.Credentials.from_service_account_file(client_file)\nclient = speech.SpeechClient(credentials=credentials)\n\naudio_file = 'interview1.wav'\nwith io.open(audio_file, 'rb') as f:\n    content = f.read()\n    audio = speech.RecognitionAudio(content=content)\n\nconfig = speech.RecognitionConfig(\n    encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n    sample_rate_hertz=16000,\n    language_code='en-US'\n)\n\nanswer = client.recognize(config=config, audio=audio)\nprint(answer)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T11:11:10.170379Z","iopub.execute_input":"2024-09-30T11:11:10.171093Z","iopub.status.idle":"2024-09-30T11:11:10.175593Z","shell.execute_reply.started":"2024-09-30T11:11:10.171051Z","shell.execute_reply":"2024-09-30T11:11:10.174477Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"## Continue the Interview","metadata":{}},{"cell_type":"code","source":"continue_question = [\n{\"role\": \"system\", \"content\": \n\"You are a helpful AI assistant.\"},\n    \n{\"role\": \"user\", \"content\": \nf\"\"\"\n{res}\n\nCandidate response:\n{answer}\n\nYou have to ask a short continue question based on candidate response.\nFormat of output would be just one new question.\n\"\"\".strip()},\n    \n]\n\noutput = pipe(continue_question, **generation_args)\ncon_res = output[0]['generated_text'].strip()\n\nlist_of_quesn.append(con_res)\n\nprint(con_res)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T11:04:23.258266Z","iopub.execute_input":"2024-09-30T11:04:23.259074Z","iopub.status.idle":"2024-09-30T11:04:25.463840Z","shell.execute_reply.started":"2024-09-30T11:04:23.259032Z","shell.execute_reply":"2024-09-30T11:04:25.462832Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"How did the implementation of Q-LoRA and 4-bit quantization specifically contribute to the accuracy of the model in the context of forest fire management?\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_question = [\n{\"role\": \"system\", \"content\": \n\"You are a helpful AI assistant.\"},\n    \n{\"role\": \"user\", \"content\": \nf\"\"\"\nList of questions:\n{list_of_quesn}\n\nYou have to ask interview a new question from the below resume which is not from the above list.\n\n{text}\nFormat of output would be just one new question.\n\"\"\".strip()},\n    \n]\n\noutput = pipe(new_question, **generation_args)\nnew_res = output[0]['generated_text'].strip()\n\nlist_of_quesn.append(new_res)\n\nprint(new_res)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T11:06:37.982222Z","iopub.execute_input":"2024-09-30T11:06:37.982662Z","iopub.status.idle":"2024-09-30T11:06:42.546783Z","shell.execute_reply.started":"2024-09-30T11:06:37.982622Z","shell.execute_reply":"2024-09-30T11:06:42.545706Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"How did the integration of Google PaLM and LangChain in the Ettara Cafe project enhance the efficiency of order placements and customer feedback processing, and what were the key challenges faced during this implementation?\n","output_type":"stream"}]},{"cell_type":"code","source":"list_of_quesn","metadata":{"execution":{"iopub.status.busy":"2024-09-30T11:07:25.616895Z","iopub.execute_input":"2024-09-30T11:07:25.617577Z","iopub.status.idle":"2024-09-30T11:07:25.623515Z","shell.execute_reply.started":"2024-09-30T11:07:25.617530Z","shell.execute_reply":"2024-09-30T11:07:25.622555Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"['What machine learning model did you fine-tune for the forest fire management system, and what was the impact on development time?',\n 'How did the implementation of Q-LoRA and 4-bit quantization specifically contribute to the accuracy of the model in the context of forest fire management?',\n 'How did the integration of Google PaLM and LangChain in the Ettara Cafe project enhance the efficiency of order placements and customer feedback processing, and what were the key challenges faced during this implementation?']"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Assessment of Interview","metadata":{}},{"cell_type":"code","source":"assessment = [\n{\"role\": \"system\", \"content\": \n\"You are a helpful AI assistant.\"},\n    \n{\"role\": \"user\", \"content\": \nf\"\"\"\nQuestion:\n{res}\n\nCandidate Response\n{answer}\n\nYou have to assess candidate response and rate them from 5 in terms of grammar, answer relevancy, and fluency.\nFormat the answer in list [grammar_score, relevancy_score, fluency] and just return list no explanation needed.\n\"\"\".strip()},\n    \n]\n\noutput = pipe(assessment, **generation_args)\nassess = output[0]['generated_text'].strip()\n\nprint(assess)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T11:14:24.589993Z","iopub.execute_input":"2024-09-30T11:14:24.590752Z","iopub.status.idle":"2024-09-30T11:14:26.316557Z","shell.execute_reply.started":"2024-09-30T11:14:24.590707Z","shell.execute_reply":"2024-09-30T11:14:26.315586Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"[grammar_score: 3, relevancy_score: 4, fluency: 3]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visual Tracking","metadata":{}},{"cell_type":"code","source":"!pip install mediapipe","metadata":{"execution":{"iopub.status.busy":"2024-09-30T11:29:45.498867Z","iopub.execute_input":"2024-09-30T11:29:45.499603Z","iopub.status.idle":"2024-09-30T11:30:01.258442Z","shell.execute_reply.started":"2024-09-30T11:29:45.499561Z","shell.execute_reply":"2024-09-30T11:30:01.257374Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":45,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting mediapipe\n  Downloading mediapipe-0.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from mediapipe) (1.4.0)\nRequirement already satisfied: attrs>=19.1.0 in /opt/conda/lib/python3.10/site-packages (from mediapipe) (23.2.0)\nRequirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from mediapipe) (24.3.25)\nRequirement already satisfied: jax in /opt/conda/lib/python3.10/site-packages (from mediapipe) (0.4.26)\nRequirement already satisfied: jaxlib in /opt/conda/lib/python3.10/site-packages (from mediapipe) (0.4.26.dev20240620)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from mediapipe) (3.7.5)\nRequirement already satisfied: numpy<2 in /opt/conda/lib/python3.10/site-packages (from mediapipe) (1.26.4)\nRequirement already satisfied: opencv-contrib-python in /opt/conda/lib/python3.10/site-packages (from mediapipe) (4.10.0.84)\nCollecting protobuf<5,>=4.25.3 (from mediapipe)\n  Downloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\nCollecting sounddevice>=0.4.4 (from mediapipe)\n  Downloading sounddevice-0.5.0-py3-none-any.whl.metadata (1.4 kB)\nRequirement already satisfied: CFFI>=1.0 in /opt/conda/lib/python3.10/site-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\nRequirement already satisfied: ml-dtypes>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from jax->mediapipe) (0.3.2)\nRequirement already satisfied: opt-einsum in /opt/conda/lib/python3.10/site-packages (from jax->mediapipe) (3.3.0)\nRequirement already satisfied: scipy>=1.9 in /opt/conda/lib/python3.10/site-packages (from jax->mediapipe) (1.14.1)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (24.1)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (10.3.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (2.9.0.post0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\nDownloading mediapipe-0.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.9/35.9 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sounddevice-0.5.0-py3-none-any.whl (32 kB)\nInstalling collected packages: protobuf, sounddevice, mediapipe\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.20.3\n    Uninstalling protobuf-3.20.3:\n      Successfully uninstalled protobuf-3.20.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\napache-beam 2.46.0 requires cloudpickle~=2.2.1, but you have cloudpickle 3.0.0 which is incompatible.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires protobuf<4,>3.12.2, but you have protobuf 4.25.5 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\ngoogle-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires protobuf<4.0.0dev,>=3.12.0, but you have protobuf 4.25.5 which is incompatible.\ngoogle-cloud-bigtable 1.7.3 requires protobuf<4.0.0dev, but you have protobuf 4.25.5 which is incompatible.\ngoogle-cloud-vision 2.8.0 requires protobuf<4.0.0dev,>=3.19.0, but you have protobuf 4.25.5 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.5 which is incompatible.\nkfp-pipeline-spec 0.2.2 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.5 which is incompatible.\ntensorflow-metadata 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.5 which is incompatible.\ntensorflow-transform 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed mediapipe-0.10.15 protobuf-4.25.3 sounddevice-0.5.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import cv2\nimport mediapipe as mp\nmp_drawing = mp.solutions.drawing_utils\nmp_drawing_styles = mp.solutions.drawing_styles\nmp_face_mesh = mp.solutions.face_mesh\n\n# For webcam input:\ndrawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n\ncap = cv2.VideoCapture(\"/kaggle/input/interview/inter1.mp4\")\n\nwith mp_face_mesh.FaceMesh(\n    max_num_faces=1,\n    refine_landmarks=True,\n    min_detection_confidence=0.5,\n    min_tracking_confidence=0.5) as face_mesh:\n    \n    while cap.isOpened():\n        success, image = cap.read()\n        if not success:\n            print(\"Ignoring empty camera frame.\")\n            break\n            \n        # If loading a video, use 'break' instead of 'continue'.\n#         continue\n\n        image.flags.writeable = False\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        results = face_mesh.process(image)\n\n        image.flags.writeable = True\n        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n        if results.multi_face_landmarks:\n            for face_landmarks in results.multi_face_landmarks:\n                mp_drawing.draw_landmarks(\n                    image=image,\n                    landmark_list=face_landmarks,\n                    connections=mp_face_mesh.FACEMESH_TESSELATION,\n                    landmark_drawing_spec=None,\n                    connection_drawing_spec=mp_drawing_styles\n                    .get_default_face_mesh_tesselation_style())\n                mp_drawing.draw_landmarks(\n                    image=image,\n                    landmark_list=face_landmarks,\n                    connections=mp_face_mesh.FACEMESH_CONTOURS,\n                    landmark_drawing_spec=None,\n                    connection_drawing_spec=mp_drawing_styles\n                    .get_default_face_mesh_contours_style())\n                mp_drawing.draw_landmarks(\n                    image=image,\n                    landmark_list=face_landmarks,\n                    connections=mp_face_mesh.FACEMESH_IRISES,\n                    landmark_drawing_spec=None,\n                    connection_drawing_spec=mp_drawing_styles\n                    .get_default_face_mesh_iris_connections_style())\n            \n        # Flip the image horizontally for a selfie-view display.\n#         cv2.imshow('MediaPipe Face Mesh', cv2.flip(image, 1))\n\n        if cv2.waitKey(5) & 0xFF == 27:\n            break\n    \n    cap.release()","metadata":{"execution":{"iopub.status.busy":"2024-09-30T12:59:03.654166Z","iopub.execute_input":"2024-09-30T12:59:03.654616Z","iopub.status.idle":"2024-09-30T12:59:03.789375Z","shell.execute_reply.started":"2024-09-30T12:59:03.654578Z","shell.execute_reply":"2024-09-30T12:59:03.788276Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"W0000 00:00:1727701143.689767     235 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\nW0000 00:00:1727701143.715662     234 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n/opt/conda/lib/python3.10/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 60\u001b[0m\n\u001b[1;32m     49\u001b[0m                 mp_drawing\u001b[38;5;241m.\u001b[39mdraw_landmarks(\n\u001b[1;32m     50\u001b[0m                     image\u001b[38;5;241m=\u001b[39mimage,\n\u001b[1;32m     51\u001b[0m                     landmark_list\u001b[38;5;241m=\u001b[39mface_landmarks,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m                     connection_drawing_spec\u001b[38;5;241m=\u001b[39mmp_drawing_styles\n\u001b[1;32m     55\u001b[0m                     \u001b[38;5;241m.\u001b[39mget_default_face_mesh_iris_connections_style())\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;66;03m# Flip the image horizontally for a selfie-view display.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m#         cv2.imshow('MediaPipe Face Mesh', cv2.flip(image, 1))\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m27\u001b[39m:\n\u001b[1;32m     61\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     cap\u001b[38;5;241m.\u001b[39mrelease()\n","\u001b[0;31merror\u001b[0m: OpenCV(4.10.0) /io/opencv/modules/highgui/src/window.cpp:1367: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvWaitKey'\n"],"ename":"error","evalue":"OpenCV(4.10.0) /io/opencv/modules/highgui/src/window.cpp:1367: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvWaitKey'\n","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-09-30T12:48:26.484301Z","iopub.execute_input":"2024-09-30T12:48:26.485141Z","iopub.status.idle":"2024-09-30T12:48:28.415740Z","shell.execute_reply.started":"2024-09-30T12:48:26.485102Z","shell.execute_reply":"2024-09-30T12:48:28.414720Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: Could not find a version that satisfies the requirement google-colab (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for google-colab\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}