{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: numpy in c:\\users\\satyam\\desktop\\ai-interview-prep\\server\\venv\\lib\\site-packages (2.1.1)\n","Requirement already satisfied: pandas in c:\\users\\satyam\\desktop\\ai-interview-prep\\server\\venv\\lib\\site-packages (2.2.3)\n","Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\satyam\\desktop\\ai-interview-prep\\server\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in c:\\users\\satyam\\desktop\\ai-interview-prep\\server\\venv\\lib\\site-packages (from pandas) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in c:\\users\\satyam\\desktop\\ai-interview-prep\\server\\venv\\lib\\site-packages (from pandas) (2024.2)\n","Requirement already satisfied: six>=1.5 in c:\\users\\satyam\\desktop\\ai-interview-prep\\server\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]},{"name":"stderr","output_type":"stream","text":["\n","[notice] A new release of pip is available: 24.0 -> 24.2\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","%pip install numpy pandas\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: PyPDF2 in c:\\users\\satyam\\desktop\\ai-interview-prep\\server\\venv\\lib\\site-packages (3.0.1)\n","Note: you may need to restart the kernel to use updated packages.\n"]},{"name":"stderr","output_type":"stream","text":["\n","[notice] A new release of pip is available: 24.0 -> 24.2\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]}],"source":["%pip install PyPDF2"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-09-30T10:43:05.049706Z","iopub.status.busy":"2024-09-30T10:43:05.049292Z","iopub.status.idle":"2024-09-30T10:43:05.297022Z","shell.execute_reply":"2024-09-30T10:43:05.296012Z","shell.execute_reply.started":"2024-09-30T10:43:05.049664Z"},"trusted":true},"outputs":[],"source":["from PyPDF2 import PdfReader"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-09-30T10:43:08.844035Z","iopub.status.busy":"2024-09-30T10:43:08.843058Z","iopub.status.idle":"2024-09-30T10:43:08.944458Z","shell.execute_reply":"2024-09-30T10:43:08.943551Z","shell.execute_reply.started":"2024-09-30T10:43:08.843989Z"},"trusted":true},"outputs":[],"source":["text = \"\"\n","pdf_reader = PdfReader('satyam_resume.pdf')\n","for page in pdf_reader.pages:\n","    text += page.extract_text()\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-09-30T10:43:09.044231Z","iopub.status.busy":"2024-09-30T10:43:09.043838Z","iopub.status.idle":"2024-09-30T10:43:09.048907Z","shell.execute_reply":"2024-09-30T10:43:09.047907Z","shell.execute_reply.started":"2024-09-30T10:43:09.044192Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Satyam Jaiswal\n","/gtbgithub.com/1SatyamJaiswal | satyam-jaiswal-b01a59189 | satyamjaiswal9752@gmail.com | /ne+91-9326021344\n","Education\n","Sardar Patel Institute of Technology Dec 2021 - May 2025\n","Bachelors of Technology in Computer Science and Engineering CGPA: 9.49\n","Kishinchand Chellaram College Jun 2018 - Mar 2020\n","HSC in PCM with Bifocal in Computer Science 90.76%\n","Work Experience\n","The Little Dentist Jun 2023 - Present\n","Software Developer, Internship Gurugram\n","•Developed a high-performance, SEO-friendly website for The Little Dentist , adding a robust patient management\n","dashboard and an ad campaign internal dashboard, resulting in a 50% increase in patient acquisition and engagement.\n","•Developed and integrated the in-app shop and secure payment feature for the Little Dentist App on Play Store and App\n","Store using Razorpay SDK .\n","•Currentlyworkingona SaaSplatformfordentalclinicsforappointmentandpatientmanagement. Thisplatformprovides\n","widgets and website solutions for clients to enhance theirdigital presence andstreamline clinicoperations .\n","•Tech Used - Nextjs, TailwindCSS, HTML, CSS, PHP, Javascript, JQuery, React Native\n","Achievements\n","Aeravat 2024 Generative AI Track Winner in this hackathon organized by CSI-IEEE SPIT 2024\n","TSEC Hacks’24 Overall2ndrunner-up and ML domain winner in this hackathon held at TSEC 2024\n","CodeOff’24 Placed2ndin this coding contest held offline at SPIT 2024\n","HackAI, Techfest’23 Overall1strunner-up in the hackathon in IIT Bombay’s Annual Technical Fest 2023\n","Codeissance’23 Placed1stin this offline hackathon, held at TSEC 2023\n","Rubix’23 Made it to the Top 10in this 48 hour national level hackathon held at TSEC 2023\n","Codechef Reached 3*on codechef ( highest rating- 1690 ). Profile: satyam9752\n","Projects\n","EvaluAid [Role: Frontend and Backend] April 2024\n","•Built a platform which aids with the evaluation of visually impaired students . Made a interface for teachers to create\n","courses, exams, objective/subjective questions and enroll students .\n","•Students can answer the questions verbally or by typing in the exam interface which is highly accessible for the visually\n","impaired. Created a pipeline which evaluates the subjective answers usingNLPandLLM. UsedReactjsandDjango,\n","DRFwithsqlite.\n","Resolvio [Role: Frontend and Backend] March 2024\n","•Developed a distributed complaint management system for streamlining the process of complaint handling efficientand\n","transparent . The system includes userandadminportal to issueandhandlethe complaints respectively.\n","•The tech stack includes Nextjsfor the client-side, NodejsandWebjsfor the server-side and Ganache as theethereum\n","network. Created a dockerised application to enhance the portability.\n","MedscanOCR [Role: Frontend and Backend] Jan 2024\n","•Developed a smart patient report management platform to convert physical reports in digitalformat for easier storage.\n","Its features are OCRfor text extraction, LLMfor data extraction and conversion in JSONformat and Vectorized Storage\n","usingchroma.\n","•Integrated data chunking to improve performance . UsedMicrosoft Phi-2 as the LLM alongwith Vision Pro for OCR and\n","Expressjs + Flask for backend with Nextjsfor the frontend.\n","Geeky Type [Role: Frontend] Jul 2023\n","•Createda responsive typeracerwebapplicationwithadjustable difficultylevels andtimedurations . Implementedfeatures\n","such assolo play andmultiplayer mode , where players are matched based on difficulty level , withreal-time progress\n","sharingusingSocket.io ,leaderboard , andprogress graphs for signed-in users.\n","•UtilizedNext.jsandTailwindCSS for the client-side, and Express.js for the server-side, with MongoDB for data storage.\n","Technical Skills\n","Programming languages: C, C++, Javascript, Python\n","Frontend: HTML, CSS, React, Next.js, React Native, Redux Toolkit, TailwindCSS, JQuery\n","Backend: Node.js, Express.js, MySQL\n","ML/AI: Numpy, Pandas, Matplotlib, Seaborn, Scikit Learn\n","Miscellaneous: Git, Github, Docker\n","Positions of Responsibility\n","Technical Head, CSI SPIT Sep 2023 - Present\n","Organized the annual hackathon and various technical workshops and competitions, managed a team of 4.\n","Technical Team, Oculus Aug 2023 - Present\n","Worked on the website of the annual cultural fest Oculus’24 made using ReactjsandTailwindCSS - link\n"]}],"source":["print(text)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["import torch"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([1.], device='cuda:0')"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# print(torch.cuda.is_available())\n","torch.ones(1).cuda()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from accelerate import infer_auto_device_map\n","\n","device_map = infer_auto_device_map()"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-09-30T10:43:13.698848Z","iopub.status.busy":"2024-09-30T10:43:13.698146Z","iopub.status.idle":"2024-09-30T10:43:29.606941Z","shell.execute_reply":"2024-09-30T10:43:29.605984Z","shell.execute_reply.started":"2024-09-30T10:43:13.698805Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\SATYAM\\Desktop\\AI-Interview-Prep\\server\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n","Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n","Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.41s/it]\n","Some parameters are on the meta device because they were offloaded to the cpu.\n"]}],"source":["import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n","\n","torch.random.manual_seed(0)\n","\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    \"microsoft/Phi-3.5-mini-instruct\",\n","    device_map=\"sequential\",\n","    torch_dtype=\"auto\",\n","    trust_remote_code=True,\n","    low_cpu_mem_usage=True,\n",")\n","tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")\n","\n","pipe = pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n",")\n","\n","generation_args = {\n","    \"max_new_tokens\": 500,\n","    \"return_full_text\": False,\n","    \"temperature\": None,\n","    \"do_sample\": False,\n","}"]},{"cell_type":"markdown","metadata":{},"source":["## Starting the Interview"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-09-30T11:03:51.609931Z","iopub.status.busy":"2024-09-30T11:03:51.609180Z","iopub.status.idle":"2024-09-30T11:03:51.614228Z","shell.execute_reply":"2024-09-30T11:03:51.613269Z","shell.execute_reply.started":"2024-09-30T11:03:51.609885Z"},"trusted":true},"outputs":[],"source":["list_of_quesn = []"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-09-30T11:03:53.035302Z","iopub.status.busy":"2024-09-30T11:03:53.034922Z","iopub.status.idle":"2024-09-30T11:03:56.352267Z","shell.execute_reply":"2024-09-30T11:03:56.351254Z","shell.execute_reply.started":"2024-09-30T11:03:53.035264Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n","You are not running the flash-attention implementation, expect numerical differences.\n"]},{"name":"stdout","output_type":"stream","text":["What programming language and framework did you use to develop the in-app shop and secure payment feature for the Little Dentist App, and how did you ensure its integration with the Play Store and App Store?\n"]}],"source":["start_question = [\n","{\"role\": \"system\", \"content\": \n","\"You are a helpful AI assistant.\"},\n","    \n","{\"role\": \"user\", \"content\": \n","f\"\"\"\n","{text}\n","\n","You have candidate resume above ask one short technical question which gives good start to interview process.\n","Format of the output has to be just one question no explaination needed.\n","\"\"\".strip()},\n","    \n","]\n","\n","output = pipe(start_question, **generation_args)\n","res = output[0]['generated_text'].strip()\n","\n","list_of_quesn.append(res)\n","\n","print(res)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-09-30T11:11:10.171093Z","iopub.status.busy":"2024-09-30T11:11:10.170379Z","iopub.status.idle":"2024-09-30T11:11:10.175593Z","shell.execute_reply":"2024-09-30T11:11:10.174477Z","shell.execute_reply.started":"2024-09-30T11:11:10.171051Z"},"trusted":true},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'google.oauth2'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moauth2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m service_account\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m speech\n\u001b[0;32m      5\u001b[0m client_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/google-audio/audio.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.oauth2'"]}],"source":["import io\n","from google.oauth2 import service_account\n","from google.cloud import speech\n","\n","client_file = '/kaggle/input/google-audio/audio.json'\n","credentials = service_account.Credentials.from_service_account_file(client_file)\n","client = speech.SpeechClient(credentials=credentials)\n","\n","audio_file = 'interview1.wav'\n","with io.open(audio_file, 'rb') as f:\n","    content = f.read()\n","    audio = speech.RecognitionAudio(content=content)\n","\n","config = speech.RecognitionConfig(\n","    encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n","    sample_rate_hertz=16000,\n","    language_code='en-US'\n",")\n","\n","answer = client.recognize(config=config, audio=audio)\n","print(answer)"]},{"cell_type":"markdown","metadata":{},"source":["## Continue the Interview"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-09-30T11:04:23.259074Z","iopub.status.busy":"2024-09-30T11:04:23.258266Z","iopub.status.idle":"2024-09-30T11:04:25.463840Z","shell.execute_reply":"2024-09-30T11:04:25.462832Z","shell.execute_reply.started":"2024-09-30T11:04:23.259032Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["I understand that the response provided doesn't give us any information about the programming language, framework, or the process of integration with the Play Store and App Store. To get a better understanding of your development process, could you please share more details about the technologies and methodologies you used to create the in-app shop and secure payment feature for the Little Dentist App? Additionally, how did you ensure its seamless integration with the Play Store and App Store?\n"]}],"source":["answer = \"I am poor.\"\n","continue_question = [\n","{\"role\": \"system\", \"content\": \n","\"You are a helpful AI assistant.\"},\n","    \n","{\"role\": \"user\", \"content\": \n","f\"\"\"\n","{res}\n","\n","Candidate response:\n","{answer}\n","\n","You have to ask a short continue question based on candidate response.\n","Format of output would be just one new question.\n","\"\"\".strip()},\n","    \n","]\n","\n","output = pipe(continue_question, **generation_args)\n","con_res = output[0]['generated_text'].strip()\n","\n","list_of_quesn.append(con_res)\n","\n","print(con_res)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-09-30T11:06:37.982662Z","iopub.status.busy":"2024-09-30T11:06:37.982222Z","iopub.status.idle":"2024-09-30T11:06:42.546783Z","shell.execute_reply":"2024-09-30T11:06:42.545706Z","shell.execute_reply.started":"2024-09-30T11:06:37.982622Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["How did the integration of Google PaLM and LangChain in the Ettara Cafe project enhance the efficiency of order placements and customer feedback processing, and what were the key challenges faced during this implementation?\n"]}],"source":["new_question = [\n","{\"role\": \"system\", \"content\": \n","\"You are a helpful AI assistant.\"},\n","    \n","{\"role\": \"user\", \"content\": \n","f\"\"\"\n","List of questions:\n","{list_of_quesn}\n","\n","You have to ask interview a new question from the below resume which is not from the above list.\n","\n","{text}\n","Format of output would be just one new question.\n","\"\"\".strip()},\n","    \n","]\n","\n","output = pipe(new_question, **generation_args)\n","new_res = output[0]['generated_text'].strip()\n","\n","list_of_quesn.append(new_res)\n","\n","print(new_res)"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-09-30T11:07:25.617577Z","iopub.status.busy":"2024-09-30T11:07:25.616895Z","iopub.status.idle":"2024-09-30T11:07:25.623515Z","shell.execute_reply":"2024-09-30T11:07:25.622555Z","shell.execute_reply.started":"2024-09-30T11:07:25.617530Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['What machine learning model did you fine-tune for the forest fire management system, and what was the impact on development time?',\n"," 'How did the implementation of Q-LoRA and 4-bit quantization specifically contribute to the accuracy of the model in the context of forest fire management?',\n"," 'How did the integration of Google PaLM and LangChain in the Ettara Cafe project enhance the efficiency of order placements and customer feedback processing, and what were the key challenges faced during this implementation?']"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["list_of_quesn"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## Assessment of Interview"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-09-30T11:14:24.590752Z","iopub.status.busy":"2024-09-30T11:14:24.589993Z","iopub.status.idle":"2024-09-30T11:14:26.316557Z","shell.execute_reply":"2024-09-30T11:14:26.315586Z","shell.execute_reply.started":"2024-09-30T11:14:24.590707Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[grammar_score: 3, relevancy_score: 4, fluency: 3]\n"]}],"source":["assessment = [\n","{\"role\": \"system\", \"content\": \n","\"You are a helpful AI assistant.\"},\n","    \n","{\"role\": \"user\", \"content\": \n","f\"\"\"\n","Question:\n","{res}\n","\n","Candidate Response\n","{answer}\n","\n","You have to assess candidate response and rate them from 5 in terms of grammar, answer relevancy, and fluency.\n","Format the answer in list [grammar_score, relevancy_score, fluency] and just return list no explanation needed.\n","\"\"\".strip()},\n","    \n","]\n","\n","output = pipe(assessment, **generation_args)\n","assess = output[0]['generated_text'].strip()\n","\n","print(assess)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## Visual Tracking"]},{"cell_type":"code","execution_count":45,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-09-30T11:29:45.499603Z","iopub.status.busy":"2024-09-30T11:29:45.498867Z","iopub.status.idle":"2024-09-30T11:30:01.258442Z","shell.execute_reply":"2024-09-30T11:30:01.257374Z","shell.execute_reply.started":"2024-09-30T11:29:45.499561Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  pid, fd = os.forkpty()\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Collecting mediapipe\n","  Downloading mediapipe-0.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n","Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from mediapipe) (1.4.0)\n","Requirement already satisfied: attrs>=19.1.0 in /opt/conda/lib/python3.10/site-packages (from mediapipe) (23.2.0)\n","Requirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from mediapipe) (24.3.25)\n","Requirement already satisfied: jax in /opt/conda/lib/python3.10/site-packages (from mediapipe) (0.4.26)\n","Requirement already satisfied: jaxlib in /opt/conda/lib/python3.10/site-packages (from mediapipe) (0.4.26.dev20240620)\n","Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from mediapipe) (3.7.5)\n","Requirement already satisfied: numpy<2 in /opt/conda/lib/python3.10/site-packages (from mediapipe) (1.26.4)\n","Requirement already satisfied: opencv-contrib-python in /opt/conda/lib/python3.10/site-packages (from mediapipe) (4.10.0.84)\n","Collecting protobuf<5,>=4.25.3 (from mediapipe)\n","  Downloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n","Collecting sounddevice>=0.4.4 (from mediapipe)\n","  Downloading sounddevice-0.5.0-py3-none-any.whl.metadata (1.4 kB)\n","Requirement already satisfied: CFFI>=1.0 in /opt/conda/lib/python3.10/site-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\n","Requirement already satisfied: ml-dtypes>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from jax->mediapipe) (0.3.2)\n","Requirement already satisfied: opt-einsum in /opt/conda/lib/python3.10/site-packages (from jax->mediapipe) (3.3.0)\n","Requirement already satisfied: scipy>=1.9 in /opt/conda/lib/python3.10/site-packages (from jax->mediapipe) (1.14.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (4.53.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (24.1)\n","Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (10.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n","Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n","Downloading mediapipe-0.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.9/35.9 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sounddevice-0.5.0-py3-none-any.whl (32 kB)\n","Installing collected packages: protobuf, sounddevice, mediapipe\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 3.20.3\n","    Uninstalling protobuf-3.20.3:\n","      Successfully uninstalled protobuf-3.20.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","apache-beam 2.46.0 requires cloudpickle~=2.2.1, but you have cloudpickle 3.0.0 which is incompatible.\n","apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\n","apache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\n","apache-beam 2.46.0 requires protobuf<4,>3.12.2, but you have protobuf 4.25.5 which is incompatible.\n","apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\n","google-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\n","google-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\n","google-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\n","google-cloud-bigquery 2.34.4 requires protobuf<4.0.0dev,>=3.12.0, but you have protobuf 4.25.5 which is incompatible.\n","google-cloud-bigtable 1.7.3 requires protobuf<4.0.0dev, but you have protobuf 4.25.5 which is incompatible.\n","google-cloud-vision 2.8.0 requires protobuf<4.0.0dev,>=3.19.0, but you have protobuf 4.25.5 which is incompatible.\n","kfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\n","kfp 2.5.0 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.5 which is incompatible.\n","kfp-pipeline-spec 0.2.2 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.5 which is incompatible.\n","tensorflow-metadata 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.5 which is incompatible.\n","tensorflow-transform 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.5 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed mediapipe-0.10.15 protobuf-4.25.3 sounddevice-0.5.0\n"]}],"source":["!pip install mediapipe"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-09-30T12:59:03.654616Z","iopub.status.busy":"2024-09-30T12:59:03.654166Z","iopub.status.idle":"2024-09-30T12:59:03.789375Z","shell.execute_reply":"2024-09-30T12:59:03.788276Z","shell.execute_reply.started":"2024-09-30T12:59:03.654578Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["W0000 00:00:1727701143.689767     235 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n","W0000 00:00:1727701143.715662     234 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n","/opt/conda/lib/python3.10/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n","  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"]},{"ename":"error","evalue":"OpenCV(4.10.0) /io/opencv/modules/highgui/src/window.cpp:1367: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvWaitKey'\n","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 60\u001b[0m\n\u001b[1;32m     49\u001b[0m                 mp_drawing\u001b[38;5;241m.\u001b[39mdraw_landmarks(\n\u001b[1;32m     50\u001b[0m                     image\u001b[38;5;241m=\u001b[39mimage,\n\u001b[1;32m     51\u001b[0m                     landmark_list\u001b[38;5;241m=\u001b[39mface_landmarks,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m                     connection_drawing_spec\u001b[38;5;241m=\u001b[39mmp_drawing_styles\n\u001b[1;32m     55\u001b[0m                     \u001b[38;5;241m.\u001b[39mget_default_face_mesh_iris_connections_style())\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;66;03m# Flip the image horizontally for a selfie-view display.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m#         cv2.imshow('MediaPipe Face Mesh', cv2.flip(image, 1))\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m27\u001b[39m:\n\u001b[1;32m     61\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     cap\u001b[38;5;241m.\u001b[39mrelease()\n","\u001b[0;31merror\u001b[0m: OpenCV(4.10.0) /io/opencv/modules/highgui/src/window.cpp:1367: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvWaitKey'\n"]}],"source":["import cv2\n","import mediapipe as mp\n","mp_drawing = mp.solutions.drawing_utils\n","mp_drawing_styles = mp.solutions.drawing_styles\n","mp_face_mesh = mp.solutions.face_mesh\n","\n","# For webcam input:\n","drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n","\n","cap = cv2.VideoCapture(\"/kaggle/input/interview/inter1.mp4\")\n","\n","with mp_face_mesh.FaceMesh(\n","    max_num_faces=1,\n","    refine_landmarks=True,\n","    min_detection_confidence=0.5,\n","    min_tracking_confidence=0.5) as face_mesh:\n","    \n","    while cap.isOpened():\n","        success, image = cap.read()\n","        if not success:\n","            print(\"Ignoring empty camera frame.\")\n","            break\n","            \n","        # If loading a video, use 'break' instead of 'continue'.\n","#         continue\n","\n","        image.flags.writeable = False\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        results = face_mesh.process(image)\n","\n","        image.flags.writeable = True\n","        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n","        if results.multi_face_landmarks:\n","            for face_landmarks in results.multi_face_landmarks:\n","                mp_drawing.draw_landmarks(\n","                    image=image,\n","                    landmark_list=face_landmarks,\n","                    connections=mp_face_mesh.FACEMESH_TESSELATION,\n","                    landmark_drawing_spec=None,\n","                    connection_drawing_spec=mp_drawing_styles\n","                    .get_default_face_mesh_tesselation_style())\n","                mp_drawing.draw_landmarks(\n","                    image=image,\n","                    landmark_list=face_landmarks,\n","                    connections=mp_face_mesh.FACEMESH_CONTOURS,\n","                    landmark_drawing_spec=None,\n","                    connection_drawing_spec=mp_drawing_styles\n","                    .get_default_face_mesh_contours_style())\n","                mp_drawing.draw_landmarks(\n","                    image=image,\n","                    landmark_list=face_landmarks,\n","                    connections=mp_face_mesh.FACEMESH_IRISES,\n","                    landmark_drawing_spec=None,\n","                    connection_drawing_spec=mp_drawing_styles\n","                    .get_default_face_mesh_iris_connections_style())\n","            \n","        # Flip the image horizontally for a selfie-view display.\n","#         cv2.imshow('MediaPipe Face Mesh', cv2.flip(image, 1))\n","\n","        if cv2.waitKey(5) & 0xFF == 27:\n","            break\n","    \n","    cap.release()"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-09-30T12:48:26.485141Z","iopub.status.busy":"2024-09-30T12:48:26.484301Z","iopub.status.idle":"2024-09-30T12:48:28.415740Z","shell.execute_reply":"2024-09-30T12:48:28.414720Z","shell.execute_reply.started":"2024-09-30T12:48:26.485102Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[31mERROR: Could not find a version that satisfies the requirement google-colab (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for google-colab\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5791113,"sourceId":9513301,"sourceType":"datasetVersion"},{"datasetId":5793406,"sourceId":9516252,"sourceType":"datasetVersion"},{"datasetId":5793647,"sourceId":9516544,"sourceType":"datasetVersion"}],"dockerImageVersionId":30776,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":4}
